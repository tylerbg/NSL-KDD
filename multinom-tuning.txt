#' ---
#' title: "Multinomial regression"
#' knit: (function(input_file, encoding) {
#'   rmarkdown::render(input_file,
#'                     encoding=encoding,
#'                     output_dir='~/NSL-KDD/results/',
#'                     output_file='multinomial.html') })
#' output:
#'   html_document:
#'     highlight: tango
#' editor_options: 
#'   chunk_output_type: console
#' ---

#+ setup, include = FALSE
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      fig.path = "results/figures/",
                      fig.show = 'hold',
                      fig.align = 'center')
knitr::opts_knit$set(root.dir = '~/NSL-KDD/')
options(width=100)

#' ## Introduction
#' 
#' Multinomial logistic regression is a type of generalized linear model (GLM) that is used to predict a categorical variable with more than two categories. It can be used to model nominal and ordinal variables and is an extension of binomial logistic regression, which is used to model binary outcomes.  Some of the advantages and disadvantages of multinomial regression include:
#' 
#' **Advantages:**
#' 
#' - Useful to predict nominal or ordinal dependent variables with more than two categories.
#' - Able to test hypotheses about the relationship between the predictor variables and the dependent variable.
#' - Can be used to estimate the relative odds of different categories, which is useful for comparing the likelihood of different outcomes.
#' 
#' **Disadvantages:**
#' 
#' - Sensitive to missing data, outliers, and multicollinearity, which can affect the estimated probabilities and coefficients.
#' - Assumes the observations are independent, which may not be the case in certain types of data such as time series data.
#' - Can have high variance if the response categories are highly imbalanced.
#' - Does not work well when the sample size is small in relation to the number of possible categories.
#' 
#' The main issue found with the NSL-KDD data set is that even though the sample data was significantly reduced by downsampling, the multinomial regression algorithm will still be slow compared to many other models.  However, the prediction metrics are quite good so that multinomial is a very useful model for intrusion detection.
#' 
#' ## Set-up
#'
#' The following set of libraries will be loaded to tune and fit a multinomial regerssion model on the NSL-KDD data set using `tidymodels`.  Specifically, the `glmnet` library is needed for the engine to fit the multinomial model. 

#+ load-libs
library(tidyverse)
library(tidymodels)
library(doParallel)
library(glmnet)
library(finetune)

options(tidymodels.dark = TRUE)

setwd("~/NSL-KDD")

# kdd_train2_ds_baked <- readRDS('data/interim/kdd_train2_ds_baked.RDS')
# 
# # Set cross-validation folds
# set.seed(4960)
# cv_folds <- vfold_cv(kdd_train2_ds_baked,
#                      v = 10)
# 
# multinom_spec <- multinom_reg(penalty = tune(),
#                               mixture = tune()) %>% 
#   set_engine("glmnet") %>% 
#   set_mode("classification") %>%
#   translate()
# 
# multinom_wf <- workflow() %>%
#   add_model(multinom_spec) %>%
#   add_formula(Class ~ .) %>%
#   add_case_weights(case_wts)
# 
# multinom_param <- multinom_spec %>% 
#   extract_parameter_set_dials()
# 
# bayes_control <- control_bayes(verbose = TRUE,
#                                verbose_iter = TRUE,
#                                no_improve = 10,
#                                # Need both below = TRUE for stacking
#                                save_pred = TRUE,
#                                save_workflow = TRUE,
#                                parallel_over = 'everything')
# 
# bayes_metrics <- metric_set(roc_auc,
#                             recall,
#                             accuracy,
#                             precision)
# 
# n_cores <- parallel::detectCores()
# 
# cl <- makeForkCluster(n_cores)
# registerDoParallel(cl)
# 
# set.seed(4960)
# multinom_bayes <- multinom_wf %>%
#   tune_bayes(resamples = cv_folds,
#              iter = 50,
#              param_info = multinom_param,
#              metrics = bayes_metrics,
#              initial = 5,
#              control = bayes_control)
# 
# stopImplicitCluster()
# 
# autoplot(multinom_bayes)
# show_best(multinom_bayes,
#           metric = 'roc_auc')
# 
# multinom_best_fit_params <- select_best(multinom_bayes,
#                                      metric = 'roc_auc')
# 
# multinom_final_wf <- multinom_wf %>%
#   finalize_workflow(multinom_best_fit_params)
# 
# multinom_final_fit <- multinom_final_wf %>%
#   fit(kdd_train2_ds_baked)
# 
# saveRDS(multinom_final_fit, 'models/tuning/multinom_fit.RDS')
# 
# rm(list = ls())
